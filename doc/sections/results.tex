\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Results}
\subsection{Classification}
\subsubsection{Logistic regression on the cancer dataset}
The classification problem with the breast cancer dataset was done by sorting the patients into two piles. One with a cancerous tumour and one without a cancerous tumour. As said in section \ref{sec:3logreg} the data is sent into the SGD algorithm. Along with the data, there is two other variables sent into the function, amount of epochs and number of mini batches. Both the two variables have an important role on the accuracy. Therefore the accuracy was plotted as functions of the two variables, showing optimum parameters without increasing the computational complexity too much. The results from the accuracy plots can be seen in figure \ref{fig:accvsepochvsmb}. This was calculated with step sizes of 10 for both epochs and mini batches in the intervals [1-150] and [1-200].





\begin{figure}[!htb] 
   \centering
   \begin{subfigure}[b]{0.80\textwidth}
      \centering
    \includegraphics[width=\textwidth]{../assets/acc_vs_mb_set_40.png}
    \caption{}
    \label{fig:accvsepoch}
   \end{subfigure}
   \quad
   \begin{subfigure}[b]{0.80\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../assets/acc_vs_epoch_set_110.png} 
    \caption{}
    \label{fig:accvsmini}
   \end{subfigure}
   \caption{In both figures the calculations have been done ten times to secure a clear overview of the trend, due to some random variables as a result of the stochastic approach. (a) Plot of the accuracy vs. the number of epochs in logistic regression with constant epoch=110 (b) Plot of the accuracy vs. the number of mini batches in logistic regression with constant mini batch=40
   }
   \label{fig:accvsepochvsmb}
\end{figure} 

Regarding the classification of the dataset, the chosen number of mini batches, and amount of epochs was set to 35 and 140 respectively. This was because the accuracy stops to increase around these values in figure \ref{fig:accvsepochvsmb}, without being too computational complex. 

The accuracy of the self written code applied to the Wisconsin breast cancer dataset with 35 mini batches and 140 epochs gave an accuracy of 0.965. The same dataset classified by scikit-learn gave an accuracy of 0.982.

Testing different amount of epochs was also done and timed to see how the increase of epochs afects the time to run the algorithms. The result of this can be seen in table \ref{tab:timetime}.


\begin{table}[H]
\centering
\caption{The time and accuracy to run the logistic regression methods with different amounts of epochs. The amount of mini batches is set to 35. The time is started when the function is called, and ends when the function is done.}
\begin{tabular}{ ccccc } 
 \toprule
  & Epochs= 140 & Epochs= 750 & Epochs= 15000 & Scikit-learn \\ 
 \midrule
 Accuracy \%  & 96.5 & 98.2 & 97.7 & 98.2\\
 
 Time (s) & 0.1562 & 0.8736 & 16.57 & 0.0030 \\ 
 \bottomrule
\end{tabular}
\label{tab:timetime}
\end{table}


\subsubsection{Neural network for classification}

Training our neural network with the MNIST dataset, we've classified handwritten digits with low success. Figure \ref{fig:mnist_predictions} shows 9 randomly selected samples and our prediction of them:

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{doc/assets/nn_sigmoid_mnist_plot.png}
    \caption{9 randomly selected samples from the MNIST testing set and our neural networks' of them.}
    \label{fig:mnist_predictions}
\end{figure}

\subsection{Regression}
\subsubsection{Stochastic gradient decent}
\Cref{fig:etas_vs_mse} depicts the MSE as a function of the learning rate. 

\Cref{fig:epochs_vs_mse} depicts the MSE as a function of the number of epochs. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{../assets/etas_vs_mse.png}
    \caption{The MSE as a function of $\log_{10}(\eta)$ for $d=3$, 1000 epochs and a batch size of 10.}
    \label{fig:etas_vs_mse}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\textwidth]{../assets/epochs_vs_mse.png}
    \caption{The MSE as a function of the number of epochs for $d=3$, learning rate of 0.1 and a batch size of 10.}
    \label{fig:epochs_vs_mse}
\end{figure}

When comparing our implemented SGD regressor to SkLearn's SGDRegressor and Ridge, we see from \cref{tab:MSE} that the lowest MSE value is obtained for SkLearn's Ridge function.

\begin{table}[!htb]
\centering
\caption{Obtained MSE values using different methods. The lowest MSE value is obtained using SkLearn's Ridge function.}
\begin{tabular}{ cccc } 
 \toprule
  & SkLearn SGDRegressor & SkLearn Ridge & SGD OLS \\ 
 \midrule
 MSE  & 0.015 & 0.008 & 0.013 \\
 \bottomrule
\end{tabular}
\label{tab:MSE}
\end{table}

\subsubsection{Neural network for regression}
Figure \ref{fig:result_nn_plots} shows the approximation of Franke's function by performing regression with the nerural network, plotted in 3D. The plots have been calculated by using different activation functions. The used design matrix goes up to degree $12$.

The activation function that performed best was the leaky RELU function, followed by the sigmoid function. The worst activation function was the ordinary RELU activation function.

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[trim=2.4cm 1cm 1.4cm 1cm, clip,width=1.1\textwidth]{../assets/nn_sigmoid_franke_plot.png}
        \caption{Sigmoid}
        \label{fig:result_ols_plot}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[trim=2.4cm 1cm 1.4cm 1cm, clip,width=1.1\textwidth]{../assets/nn_relu_franke_plot.png}
        \caption{RELU}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[trim=2.4cm 1cm 1.4cm 1cm, clip,width=1.1\textwidth]{../assets/nn_leaky_franke_plot.png}
        \caption{Leaky RELU}
    \end{subfigure}
    \caption{Prediction of Franke' function using a neural network, with different activation functions.}
    \label{fig:result_nn_plots}
\end{figure}




\end{document}
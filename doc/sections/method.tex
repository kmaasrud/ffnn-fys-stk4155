\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Method}
\subsection{datasets}
It is always useful to study different datasets and see how different methods work on different datasets. That is why in this article, we study three different datasets. The first dataset is generated from the two-dimensional Franke's function. The second dataset is the well known MNIST dataset, while the last dataset is the Wisconsin cancer data which can be solved as a binary problem.

\subsubsection{Franke's function}
The dataset occuring from Franke's function is produced and implemented the same way as explained in the theory and \cite{project1}.

\subsubsection{MNIST}
The famous MNIST dataset is a collection of handwritten numbers, as $28\times 28$ grayscale images. It comes in two sets, a training set with $60,000$ images, and a testing set with $10,000$ images. In this report, we will model the inputs as a $28\times 28 = 784$-dimensional vector, and the output as a $10$-dimensional state vector, with each dimension representing the corresponding digit.

\subsubsection{Wisconsin cancer data}
The Wisconsin cancer dataset is a dataset providing 30 predictors for 569 patients each. Of the 569 patients, 357 possess a cancerous tumour while the 212 remaining patients do not possess cancerous tumors.

The dataset is easily accessed using scikit-learn. The design matrix is a 569x30 matrix while the target vector is a vector containing 569 binary values. A patient having the target value 1 represents a cancerous tumour, while the value 0 represents a non cancerous tumour. More info on how to access and use the dataset can be studied at \cite{scikit-learn}.

\subsection{Logistic regression}\label{sec:3logreg}
The implementation of the logistic regression is implemented pretty straight forward. The logistic regression functions is set up in a class containing the most important functions like the SGD, a prediction function and the learning rate.

The clue within machine learning algorithms is the need to train a model. The whole dataset is naturally split into test and training parts using standard scikit-learn functions. This is followed by a scaling of the data by subtracting the mean to adjust for data points with irregular values, using the same library. In the logistic regression class, the training dataset used to train the model is sent into the SGD function within the class. After the training is done, the prediction is ready to be set in motion. The test data is sent into the prediction function, which returns a predicted response vector for the dataset. Which then can be used to calculate the accuracy.

Logistic regression was also implemented and performed on the same dataset using nothing but the scikit-learn library. This was to see how close the self written code can compare to the regression accuracy achieved by scikit-learn.

\subsection{Neural network}
The neural network is implemented as a class containing important algorithms like the training algorithm SGD, back propagation and feed forward function - that feeds forward the input $X$ as well as storing the layer's activations.

The algorithms are implemented according to the theory about neural networks. The neural network is also implemented with the option to choose the amount of layers and neurons within each layer. This is crucial for switching between datasets, where different layer structures might be necessary to optimize the convergence rate of the predictions.

The neural network has a variety of different variables to be chosen. The parameters are the amount of epochs, the size of the mini-batches, the learning rate, which activation function to use, which cost function to use and the activation function for the last layer.
\end{document}


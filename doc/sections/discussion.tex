\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Discussion}
\subsection{Linear regression performed on Franke's function}
Linear regression was performed on the Franke's function dataset by using a neural network and different activation functions. The leaky RELU as an activation function performed best which is understandable thinking about the fact that the sigmoid is a logisitc function, while the RELU is often used for regression.

The regression done by the neural network was a bit worse than the results achieved in project 1. To see the results from project 1 please have a look here \cite{project1}. In that project the regression was done by ordinary least squares and ridge regression. 


\subsection{Logistic regression compared to scikit-learn}
After testing the logistic regression code and compared it to the logistic regression done by scikit-learn, it shows that that the difference is quiet small. The logistic regression done by scikit-learn managed to classify 98.2\% of the data into the right class. On the other hand the self written algorithms managed to get 96.5\% of the cases right, which is quiet good. Keeping in mind that the whole dataset consists of 569 patients, making the self written code placing 549 of the patients into the right class. Scikit-learns 98.2\% made 559 correct choices which is only 10 more. Even though the accuracy of 96.5\% was not the best accuracy possible, it is assumed in this paragraph because the amount of epochs and mini batches was chosen as a optimum choice regarding the computational time and accuracy. 

It is possible to increase the accuracy by using more computational effort, a possible way of doing this is to increase the number of epochs, which makes the data go through the model more times. Since the cancer dataset only consists of 569 patients, this opened up the possibility of testing the effects of increasing the amount of epochs up to a ridiculous amount. In table \ref{tab:timetime} it is easy to see that increasing the epochs, increases the accuracy up to a certain sweet spot, then the accuracy decreases because the model is overfitting, which can be read more about in \cite{project1}. It is also important to notice the large increase in computational time when increasing the epochs. By increasing the 140 epochs by a factor of 5.4 and 107, the time increased by a factor of 5.6 and 106 respectively, which makes the time approximately proportional with the amount of epochs. Even though the best result was achieved by increasing the time by a factor of 5, this increase could get very large when working with a large dataset. It is also important to notice that the fast execution of scikit-learns calculations, probably was due to the dataset being really small.


\end{document}
\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Discussion}
\subsection{Linear regression performed on Franke's function}
Linear regression was performed on the Franke's function dataset by using a neural network and different activation functions. The leaky RELU as an activation function performed best which is understandable thinking about the fact that the sigmoid is a logisitc function, while the RELU is often used for regression.

The regression done by the neural network was a bit worse than the results achieved in project 1. To see the results from project 1 please have a look here \cite{project1}. In project 1 the regression was done by ordinary least squares and ridge regression. Then the parameters were optimized to give the best possible result. The fine thing about numerical algorithms is that approximately all algorithms have the possibility to fine tune parameters and increase the computer power to make the algorithm achieve better results. The case is the same for this neural network, for example there would be possible to add more layers and nodes to make the neural network perform better. 


\subsection{Logistic regression compared to scikit-learn}
After testing the logistic regression code and compared it to the logistic regression done by scikit-learn, it shows that that the difference is quite small. The logistic regression done by scikit-learn managed to classify 98.2\% of the data into the right class. On the other hand the self written algorithms managed to get 96.5\% of the cases right, which is quite good. Keeping in mind that the whole dataset consists of 569 patients, making the self written code placing 549 of the patients into the right class. Scikit-learns 98.2\% made 559 correct choices which is only 10 more. Even though the accuracy of 96.5\% was not the best accuracy possible, it is assumed in this paragraph because the amount of epochs and mini batches was chosen as a optimum choice regarding the computational time and accuracy. 

It is possible to increase the accuracy by using more computational effort, a possible way of doing this is to increase the number of epochs, which makes the data go through the model more times. Since the cancer dataset only consists of 569 patients, this opened up the possibility of testing the effects of increasing the amount of epochs up to a ridiculous amount. In table \ref{tab:timetime} it is easy to see that increasing the epochs, increases the accuracy up to a certain sweet spot, then the accuracy decreases because the model is overfitting, which can be read more about in \cite{project1}. It is also important to notice the large increase in computational time when increasing the epochs. By increasing the 140 epochs by a factor of 5.4 and 107, the time increased by a factor of 5.6 and 106 respectively, which makes the time approximately proportional with the amount of epochs. Even though the best result was achieved by increasing the time by a factor of 5, this increase could get very large when working with a large dataset. It is also important to notice that the fast execution of scikit-learns calculations, probably was due to the dataset being really small.

\subsection{Classification of MNIST dataset, using neural network}
Due to time-constraints, we did not get to analyze our NN's performance with the MNIST dataset and find the ideal network structure. As a consequence, the results of our classification were lamentable, to say the least. The network seems to be classifying 1s, 4s and 7s pretty well, but this only anecdotal evidence. Given more time, we would probably utilize a more optimized (and importantly, speedy) implementation of the feed-forward neural network to find an ideal layer structure before using it on our own - much slower - neural network code. This would also allow us to do proper validation of our predictions and see how fast they converge in accuracy.


\end{document}